								<div class="image fit captioned align-just">
                  <div class="videocontainer">
                  <video controls class="videothing">
                   <source src="images/32-oral.mp4" type="video/mp4">
                   Your browser does not support the video tag.
                  </video>
                  </div>
                  <h3>Overview</h3>
                  Our oral presentation for the
                  <a href="http://visual.cs.brown.edu/aicc2020/">AI for Content Creation
                  Workshop</a>.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/overview.png">
									<img src="images/overview.png" alt="" />
                  </a>
                  <h3>Bert-to-BigGAN transfer</h3>
Our approach enables translation between fixed off-the-shelve expert models
such as BERT and BigGAN without having to modify or finetune them.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/exemplarguided.jpg">
									<img src="images/exemplarguided.jpg" alt="" />
                  </a>
                  <h3>Exemplar-Guided Image Synthesis</h3>
By combining the segmentation of an image x, with the invariances obtained from
an exemplar image y, our approach fuses a segmentation network and an
autoencoder to enable exemplar-guided image synthesis.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/costs.jpg">
									<img src="images/costs.jpg" alt="" />
                  </a>
                  <h3>Computational Cost and Energy Consumption</h3>
We compare computational costs of our conditional INN to those of <a
href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow/performance">BERT</a>,
<a href="https://github.com/ajbrock/BigGAN-PyTorch">BigGAN</a> and <a
href="https://github.com/NVlabs/FUNIT/">FUNIT</a>. Once strong domain experts
are available, they can be repurposed by our approach in a time-, energy- and
cost-effective way. With training costs of our conditional INN being two orders
of magnitude smaller than the training costs of the domain experts, the latter
are amortized over all the new tasks that can be solved by fusing experts with
our approach.

Energy consumption of a Titan X is based on the <a
href="https://www.nvidia.com/en-us/geforce/products/10series/titan-x-pascal/">recommended
system power</a> (0.6 kW) by NVIDIA, and energy consumption of eight V100 on
the <a
href="https://docs.nvidia.com/dgx/dgx1-user-guide/introduction-to-dgx1.html">power</a>
(3.5 kW) of a NVIDIA DGX-1 system. Costs are based on the <a
href="https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Electricity_price_statistics">average
price</a> of 0.216 EUR per kWh in the EU, and CO2 emissions on the <a
href="https://www.eea.europa.eu/data-and-maps/daviz/co2-emission-intensity-5">average
emissions</a> of 0.296 kg CO2 per kWh in the EU.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/landscapes_layers.jpg">
									<img src="images/landscapes_layers.jpg" alt="" />
                  </a>
                  <h3>Insights into learned invariances</h3>
Translating different layers of an expert model to the representation of an
autoencoder reveals its learned invariances and thus provides
diagnostic insights. Here, the expert is a segmentation model, and the visualizations
demonstrate how its internal representations become increasingly
invariant against style and appearance.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/stylizedvsvanilla.jpg">
									<img src="images/stylizedvsvanilla.jpg" alt="" />
                  </a>
                  <h3>Sketch-to-Image Transfer</h3>
When trained on a <a href="https://github.com/rgeirhos/Stylized-ImageNet">stylized version of
ImageNet</a> to remove its texture bias, a ResNet-50 classifier can be fused
with a <a href="https://github.com/ajbrock/BigGAN-PyTorch">BigGAN generator</a>
to produce coherent sketch-to-image synthesis results (left), whereas a vanilla
ResNet-50 fails to produce meaningful representations for sketches (right).
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/unsupervised.jpg">
									<img src="images/unsupervised.jpg" alt="" />
                  </a>
                  <h3>Unsupervised disentangling of shape and appearance</h3>
Training our approach on synthetically deformed images, our conditional INN
learns to extract a disentangled shape representation v from y, which can be
recombined with arbitrary appearances obtained from x.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/landscapes.jpg">
									<img src="images/landscapes.jpg" alt="" />
                  </a>
                  <h3>Layout-guided image synthesis</h3>
The ability to sample the invariances v enables generation of diverse and
realistic images which are consistent with a given label map.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/imgmod.jpg">
									<img src="images/imgmod.jpg" alt="" />
                  </a>
									<h3>Face Attribute Modification</h3>
                  Compared to
                  <a href="https://github.com/yunjey/StarGAN">StarGAN</a>, our approach produces
                  more coherent changes, e.g. changes in gender cause changes in hair length and
                  changes in the beard attribute have no effect on female faces. This demon-
                  strates the advantage of fusing attribute information on a low-dimensional
                  representation of a generic autoencoder.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/txt2img_large.jpg">
									<img src="images/txt2img_small.jpg" alt="" />
                  </a>
									<h3>Text-to-Image</h3>
                  Fusing text representations of BERT with a BigGAN image
                  generator enables text-to-image generation. Results show a high diversity of
                  samples, the ability for fine-grained control of the generation process, e.g.
                  color in the first two rows, and a large diversity in the objects that can be
                  synthesized, e.g. school buses and broccoli plants.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/stylizedtoimg.jpg">
									<img src="images/stylizedtoimg.jpg" alt="" />
                  </a>
									<h3>Edge-to-Image</h3>
                  Edge-to-Image Synthesis requires a good representation of
                  edges, which can be obtained from a Stylized ResNet-50.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/vanillatoimg.jpg">
									<img src="images/vanillatoimg.jpg" alt="" />
                  </a>
									<h3>Inpainting</h3>
                  Inpainting Tasks require a good representation of the
                  textures, which can be obtained from a standard ResNet-50.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/argmaxtoimg.jpg">
									<img src="images/argmaxtoimg.jpg" alt="" />
                  </a>
									<h3>Segmentation-to-Image</h3>
                  Fusing argmaxed predictions (left) of a segmentation expert with a
                  decoder produces diverse outputs (right) for
                  segmentation-to-image tasks.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/logitstoimg.jpg">
									<img src="images/logitstoimg.jpg" alt="" />
                  </a>
									<h3>Controlling Output Diversity</h3>
                  Logits of a segmentation expert (visualized on the left with a random
                                                   projection to RGB) contain more information
                  about the input, resulting in less diverse outputs for shape-guided image synthesis.
								</div>
==========
								<div class="image fit captioned align-just">
                  <a href="images/unalignedimg2img.jpg">
									<img src="images/unalignedimg2img.jpg" alt="" />
                  </a>
									<h3>Unaligned Image-to-Image Translation</h3>
                  As our approach does not require gradients of the domain
                  experts, we can also directly use labels obtained from human experts to perform
                  unaligned image-to-image translation between different domains.
								</div>
